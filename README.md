# easy-beam-pipeline
A minimal Apache Beam pipeline that can be run both locally and on Google Cloud Dataflow.
This repostitory can be used as a template for testing various functionalities of Apache Beam python SDK.

**Motivation & Goals**:
- to provide a minimal apache-beam setup preloaded with sample data which users can customize and add custom logics to in order to test things out.
- (my motivation:) to learn apache-beam's basic concepts and python SDK

<br>

## Usage
---
### Run it locally (DirectRunner)

<br>

1. Install the dependencies which is just apache-beam library to run the sample script.
```bash
pip install apache-beam[gcp]

# with zsh
pip install 'apache-beam[gcp]'
```
*if you use ```zsh``` you might have to put ```apache-beam[gcp]``` in quotation marks.

<br>

2. Generate some sample logs with the included utility script ```generate_logs.py``` .
```
usage: generate_logs.py [-h] [--csv] [-H] [--json] num_records

positional arguments:
  num_records   number of records to generate

optional arguments:
  -h, --help    show this help message and exit
  --csv         write records to csv
  -H, --header  add header to csv
  --json        write records to json
```

<br>

3. Add custom user-defined functions and transformations

Under the ```transformations``` directory, you can store custom functions and transformations in each respective module, which can be directory imported in ```beam_local.py```
```
└── transformations
   ├── __init__.py
   ├── functions.py     # stores apache_beam.DoFn classes
   └── ptransforms.py   # stores apache_beam.PTransform classes
```

<br>

4. Run the apache-beam mini pipeline locally
```
python beam_local.py
```

<br>

### Run it with Google Cloud Dataflow (DataflowRunner)
<br>

1. Prepare Google Cloud development environment (CLI and IAM) and enable DataFlowAPI
2. Store sample logs generated by ```generate_logs.py``` in a google cloud storage bucket
3. Add custom user-defined functions and transformations
Just like in the local example, under the ```transformations``` directory, you can store custom functions and transformations in each respective module, which can be directory imported in ```beam_dataflow.py```
```
└── transformations
   ├── __init__.py
   ├── functions.py     # stores apache_beam.DoFn classes
   └── ptransforms.py   # stores apache_beam.PTransform classes
```

*NOTE: Behaviors differ from the DirectRunner(local) when you run a apache-beam pipeline with DataFlow runner. For instance, the local example directly reads data from / write data to files stored in the host machine, which cannot be done the same way in Dataflow. With Dataflow, you need to specify storage locations within Google Cloud and you need to use ```apache_beam.io.ReadFromText``` or ```apache_beam.io.WriteToText``` instead of reading and writing directly.

4. Run the pipeline with Dataflow parameters.
Dataflow expects a handful of parameters when you run the pipeline. Here is a minimal example you can use to run the pipeline.
```
python beam_dataflow.py \
    --runner DataflowRunner \
    --project <your-gcp-project> \
    --input gs://<path_to_input_file>
    --output gs://<path_to_output_dir>
    --temp_location gs://<path>/tmp/ \
    --region <gcp_region_of_your_choice> \
    --setup_file ./setup.py
``` 